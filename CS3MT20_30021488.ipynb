{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_tvQqO5EYMz"
      },
      "source": [
        "# Step 1: Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhgYu18FM-_L"
      },
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDjTTcDrtVl"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvU6i2KNHzC4",
        "outputId": "a6b98467-d448-40a1-eb4c-210b6ac04992"
      },
      "source": [
        "index=input('type your student number?')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type your student number?30021488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=divmod(int(index),4)\n",
        "yourdata1=x[1]\n",
        "y=divmod(int(index),3)\n",
        "yourdata2=y[1]\n",
        "\n",
        "print('This is your data set index ----> (', x[1], y[1], ')' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8Gmf-HdGcWV",
        "outputId": "9857a8eb-53ff-4ca9-ff5c-9926c0234d3e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is your data set index ----> ( 0 2 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXPpVRSGAPM7",
        "outputId": "9a56825e-024b-4d42-9c2f-4d4d41f92b15"
      },
      "source": [
        "data1= twenty_train.target_names[x[1]]\n",
        "data2= twenty_train.target_names[y[1]]\n",
        "categories1=[data1,data2]\n",
        "print(categories1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'sci.med']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1jAHpjtaSPu"
      },
      "source": [
        "# Step 2 Process your text data, extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_vU9NocxVC"
      },
      "source": [
        "# 2.1 An example of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = twenty_train.data[0]\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U75ATBUwEvzf",
        "outputId": "b8a1b2ca-e76e-4df0-cf23-f61a4d7b9ef2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n",
            "Organization: The City University\n",
            "Lines: 14\n",
            "\n",
            "Does anyone know of a good way (standard PC application/PD utility) to\n",
            "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
            "do the same, converting to HPGL (HP plotter) files.\n",
            "\n",
            "Please email any response.\n",
            "\n",
            "Is this the correct group?\n",
            "\n",
            "Thanks in advance.  Michael.\n",
            "-- \n",
            "Michael Collier (Programmer)                 The Computer Unit,\n",
            "Email: M.P.Collier@uk.ac.city                The City University,\n",
            "Tel: 071 477-8000 x3769                      London,\n",
            "Fax: 071 477-8565                            EC1V 0HB.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoQ1mz12N4sJ",
        "outputId": "bc548584-8d89-4458-fc79-2c974cff542b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize: search: nltk tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "example_tokenize = word_tokenize(dataset)\n",
        "print(\"-------------------------tokenize:\")\n",
        "print(example_tokenize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9am-ciHNw1_",
        "outputId": "d4181a08-2e73-4f48-c777-9e64c239e963"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------tokenize:\n",
            "['From', ':', 'sd345', '@', 'city.ac.uk', '(', 'Michael', 'Collier', ')', 'Subject', ':', 'Converting', 'images', 'to', 'HP', 'LaserJet', 'III', '?', 'Nntp-Posting-Host', ':', 'hampton', 'Organization', ':', 'The', 'City', 'University', 'Lines', ':', '14', 'Does', 'anyone', 'know', 'of', 'a', 'good', 'way', '(', 'standard', 'PC', 'application/PD', 'utility', ')', 'to', 'convert', 'tif/img/tga', 'files', 'into', 'LaserJet', 'III', 'format', '.', 'We', 'would', 'also', 'like', 'to', 'do', 'the', 'same', ',', 'converting', 'to', 'HPGL', '(', 'HP', 'plotter', ')', 'files', '.', 'Please', 'email', 'any', 'response', '.', 'Is', 'this', 'the', 'correct', 'group', '?', 'Thanks', 'in', 'advance', '.', 'Michael', '.', '--', 'Michael', 'Collier', '(', 'Programmer', ')', 'The', 'Computer', 'Unit', ',', 'Email', ':', 'M.P.Collier', '@', 'uk.ac.city', 'The', 'City', 'University', ',', 'Tel', ':', '071', '477-8000', 'x3769', 'London', ',', 'Fax', ':', '071', '477-8565', 'EC1V', '0HB', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemmer: search: nltk stemmer\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "example_stem = [stemmer.stem(word) for word in example_tokenize]\n",
        "print(\"-------------------------stem:\")\n",
        "print(example_stem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blhCVOY4N8V2",
        "outputId": "5cbdb50c-2e9c-4405-cfbd-17322331e631"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------stem:\n",
            "['from', ':', 'sd345', '@', 'city.ac.uk', '(', 'michael', 'collier', ')', 'subject', ':', 'convert', 'imag', 'to', 'hp', 'laserjet', 'iii', '?', 'nntp-posting-host', ':', 'hampton', 'organ', ':', 'the', 'citi', 'univers', 'line', ':', '14', 'doe', 'anyon', 'know', 'of', 'a', 'good', 'way', '(', 'standard', 'pc', 'application/pd', 'util', ')', 'to', 'convert', 'tif/img/tga', 'file', 'into', 'laserjet', 'iii', 'format', '.', 'we', 'would', 'also', 'like', 'to', 'do', 'the', 'same', ',', 'convert', 'to', 'hpgl', '(', 'hp', 'plotter', ')', 'file', '.', 'pleas', 'email', 'ani', 'respons', '.', 'is', 'thi', 'the', 'correct', 'group', '?', 'thank', 'in', 'advanc', '.', 'michael', '.', '--', 'michael', 'collier', '(', 'programm', ')', 'the', 'comput', 'unit', ',', 'email', ':', 'm.p.collier', '@', 'uk.ac.c', 'the', 'citi', 'univers', ',', 'tel', ':', '071', '477-8000', 'x3769', 'london', ',', 'fax', ':', '071', '477-8565', 'ec1v', '0hb', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_tagging: search: nltk pos tagging example\n",
        "example_posTag = nltk.pos_tag(example_tokenize)\n",
        "print(\"-------------------------pos_tagging:\")\n",
        "print(example_posTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awHY-0sUN-A9",
        "outputId": "c878302c-bd32-44ee-a699-f4948622cd36"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------pos_tagging:\n",
            "[('From', 'IN'), (':', ':'), ('sd345', 'NN'), ('@', 'NN'), ('city.ac.uk', 'NN'), ('(', '('), ('Michael', 'NNP'), ('Collier', 'NNP'), (')', ')'), ('Subject', 'NN'), (':', ':'), ('Converting', 'NN'), ('images', 'NNS'), ('to', 'TO'), ('HP', 'NNP'), ('LaserJet', 'NNP'), ('III', 'NNP'), ('?', '.'), ('Nntp-Posting-Host', 'NN'), (':', ':'), ('hampton', 'NN'), ('Organization', 'NN'), (':', ':'), ('The', 'DT'), ('City', 'NNP'), ('University', 'NNP'), ('Lines', 'NNPS'), (':', ':'), ('14', 'CD'), ('Does', 'NNP'), ('anyone', 'NN'), ('know', 'NN'), ('of', 'IN'), ('a', 'DT'), ('good', 'JJ'), ('way', 'NN'), ('(', '('), ('standard', 'JJ'), ('PC', 'NN'), ('application/PD', 'JJ'), ('utility', 'NN'), (')', ')'), ('to', 'TO'), ('convert', 'VB'), ('tif/img/tga', 'JJ'), ('files', 'NNS'), ('into', 'IN'), ('LaserJet', 'NNP'), ('III', 'NNP'), ('format', 'NN'), ('.', '.'), ('We', 'PRP'), ('would', 'MD'), ('also', 'RB'), ('like', 'VB'), ('to', 'TO'), ('do', 'VB'), ('the', 'DT'), ('same', 'JJ'), (',', ','), ('converting', 'VBG'), ('to', 'TO'), ('HPGL', 'NNP'), ('(', '('), ('HP', 'NNP'), ('plotter', 'NN'), (')', ')'), ('files', 'NNS'), ('.', '.'), ('Please', 'NNP'), ('email', 'VBZ'), ('any', 'DT'), ('response', 'NN'), ('.', '.'), ('Is', 'VBZ'), ('this', 'DT'), ('the', 'DT'), ('correct', 'JJ'), ('group', 'NN'), ('?', '.'), ('Thanks', 'NNS'), ('in', 'IN'), ('advance', 'NN'), ('.', '.'), ('Michael', 'NNP'), ('.', '.'), ('--', ':'), ('Michael', 'NNP'), ('Collier', 'NNP'), ('(', '('), ('Programmer', 'NNP'), (')', ')'), ('The', 'DT'), ('Computer', 'NNP'), ('Unit', 'NNP'), (',', ','), ('Email', 'NNP'), (':', ':'), ('M.P.Collier', 'NNP'), ('@', 'NNP'), ('uk.ac.city', 'NN'), ('The', 'DT'), ('City', 'NNP'), ('University', 'NNP'), (',', ','), ('Tel', 'NNP'), (':', ':'), ('071', 'CD'), ('477-8000', 'CD'), ('x3769', 'JJ'), ('London', 'NNP'), (',', ','), ('Fax', 'NNP'), (':', ':'), ('071', 'CD'), ('477-8565', 'JJ'), ('EC1V', 'NNP'), ('0HB', 'CD'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constituency parsing, chunking\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(example_posTag)\n",
        "print(\"-------------------------constituency parsing:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89cPbffiOiDt",
        "outputId": "006345e3-b28a-4553-d7e3-4f6a35fe5a38"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------constituency parsing:\n",
            "(S\n",
            "  From/IN\n",
            "  :/:\n",
            "  (NP sd345/NN)\n",
            "  (NP @/NN)\n",
            "  (NP city.ac.uk/NN)\n",
            "  (/(\n",
            "  Michael/NNP\n",
            "  Collier/NNP\n",
            "  )/)\n",
            "  (NP Subject/NN)\n",
            "  :/:\n",
            "  (NP Converting/NN)\n",
            "  images/NNS\n",
            "  to/TO\n",
            "  HP/NNP\n",
            "  LaserJet/NNP\n",
            "  III/NNP\n",
            "  ?/.\n",
            "  (NP Nntp-Posting-Host/NN)\n",
            "  :/:\n",
            "  (NP hampton/NN)\n",
            "  (NP Organization/NN)\n",
            "  :/:\n",
            "  The/DT\n",
            "  City/NNP\n",
            "  University/NNP\n",
            "  Lines/NNPS\n",
            "  :/:\n",
            "  14/CD\n",
            "  Does/NNP\n",
            "  (NP anyone/NN)\n",
            "  (NP know/NN)\n",
            "  of/IN\n",
            "  (NP a/DT good/JJ way/NN)\n",
            "  (/(\n",
            "  (NP standard/JJ PC/NN)\n",
            "  (NP application/PD/JJ utility/NN)\n",
            "  )/)\n",
            "  to/TO\n",
            "  convert/VB\n",
            "  tif/img/tga/JJ\n",
            "  files/NNS\n",
            "  into/IN\n",
            "  LaserJet/NNP\n",
            "  III/NNP\n",
            "  (NP format/NN)\n",
            "  ./.\n",
            "  We/PRP\n",
            "  would/MD\n",
            "  also/RB\n",
            "  like/VB\n",
            "  to/TO\n",
            "  do/VB\n",
            "  the/DT\n",
            "  same/JJ\n",
            "  ,/,\n",
            "  converting/VBG\n",
            "  to/TO\n",
            "  HPGL/NNP\n",
            "  (/(\n",
            "  HP/NNP\n",
            "  (NP plotter/NN)\n",
            "  )/)\n",
            "  files/NNS\n",
            "  ./.\n",
            "  Please/NNP\n",
            "  email/VBZ\n",
            "  (NP any/DT response/NN)\n",
            "  ./.\n",
            "  Is/VBZ\n",
            "  this/DT\n",
            "  (NP the/DT correct/JJ group/NN)\n",
            "  ?/.\n",
            "  Thanks/NNS\n",
            "  in/IN\n",
            "  (NP advance/NN)\n",
            "  ./.\n",
            "  Michael/NNP\n",
            "  ./.\n",
            "  --/:\n",
            "  Michael/NNP\n",
            "  Collier/NNP\n",
            "  (/(\n",
            "  Programmer/NNP\n",
            "  )/)\n",
            "  The/DT\n",
            "  Computer/NNP\n",
            "  Unit/NNP\n",
            "  ,/,\n",
            "  Email/NNP\n",
            "  :/:\n",
            "  M.P.Collier/NNP\n",
            "  @/NNP\n",
            "  (NP uk.ac.city/NN)\n",
            "  The/DT\n",
            "  City/NNP\n",
            "  University/NNP\n",
            "  ,/,\n",
            "  Tel/NNP\n",
            "  :/:\n",
            "  071/CD\n",
            "  477-8000/CD\n",
            "  x3769/JJ\n",
            "  London/NNP\n",
            "  ,/,\n",
            "  Fax/NNP\n",
            "  :/:\n",
            "  071/CD\n",
            "  477-8565/JJ\n",
            "  EC1V/NNP\n",
            "  0HB/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7VpnVNpKuUt"
      },
      "source": [
        "#2.2 NLP Preprocesssing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "stopwordEn = stopwords.words('english')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "def lemmaWord(word):\n",
        "    lemma = wordnet.morphy(word)\n",
        "    if lemma is not None:\n",
        "        return lemma\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def stemWord(word):\n",
        "    stem = stemmer.stem(word)\n",
        "    if stem is not None:\n",
        "        return stem\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def processText(text, lemma=False, gram=1, rmStop=True):  # default remove stop words\n",
        "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE)  # delete URL, #hashtag# , and @xxx\n",
        "    tokens = word_tokenize(text)\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    new_tokens = []\n",
        "    stoplist = stopwordEn if rmStop else []\n",
        "    for i in tokens:\n",
        "        i = i.lower()\n",
        "        if i.isalpha() and (i not in stoplist or i in whitelist):  # i not in ['.',',',';']  and (...)\n",
        "            if lemma: i = lemmaWord(i)\n",
        "            new_tokens.append(i)\n",
        "    del tokens\n",
        "    if gram <= 1:\n",
        "        return new_tokens\n",
        "    else:\n",
        "        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ies4pOWDQGc4",
        "outputId": "ade04706-656e-4ad4-d528-0b8a09a8f053"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTags(text):\n",
        "    token = word_tokenize(text)\n",
        "    token = [l.lower() for l in token]\n",
        "    train_tags = nltk.pos_tag(token)\n",
        "    return [i[1] for i in train_tags]"
      ],
      "metadata": {
        "id": "1MKRthOcQsrL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processText(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e82vGC6Qu57",
        "outputId": "22c6a9a7-93ed-48de-9a02-cd625d5093e6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['michael', 'collier', 'subject', 'converting', 'images', 'hp', 'laserjet', 'iii', 'hampton', 'organization', 'city', 'university', 'lines', 'anyone', 'know', 'good', 'way', 'standard', 'pc', 'utility', 'convert', 'files', 'laserjet', 'iii', 'format', 'would', 'also', 'like', 'converting', 'hpgl', 'hp', 'plotter', 'files', 'please', 'email', 'response', 'correct', 'group', 'thanks', 'advance', 'michael', 'michael', 'collier', 'programmer', 'computer', 'unit', 'email', 'city', 'university', 'tel', 'london', 'fax']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getTags(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F7p5NMbQyOV",
        "outputId": "25ec224c-8b3a-4fad-c06c-af2fdf771941"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IN', ':', 'NN', 'NN', 'NN', '(', 'FW', 'NN', ')', 'NN', ':', 'NN', 'NNS', 'TO', 'VB', 'NN', 'NN', '.', 'NN', ':', 'NN', 'NN', ':', 'DT', 'NN', 'NN', 'NNS', ':', 'CD', 'VBZ', 'NN', 'VB', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'NN', 'JJ', 'NN', ')', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'NN', '.', 'PRP', 'MD', 'RB', 'VB', 'TO', 'VB', 'DT', 'JJ', ',', 'VBG', 'TO', 'VB', '(', 'JJ', 'NN', ')', 'NNS', '.', 'VB', 'JJ', 'DT', 'NN', '.', 'VBZ', 'DT', 'DT', 'JJ', 'NN', '.', 'NNS', 'IN', 'NN', '.', 'NN', '.', ':', 'JJ', 'NN', '(', 'NN', ')', 'DT', 'NN', 'NN', ',', 'NN', ':', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NN', ',', 'NN', ':', 'CD', 'CD', 'JJ', 'NN', ',', 'NN', ':', 'CD', 'JJ', 'NN', 'CD', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44xTvpLa_UC9"
      },
      "source": [
        "# Step 3: Build a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(categories1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIVLVgc1Rd_W",
        "outputId": "59a46343-5ea0-4157-a7aa-bc95a4552b22"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'sci.med']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['alt.atheism', 'sci.med']"
      ],
      "metadata": {
        "id": "Q3iFKcV-UzSN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the original dataset with the two-class dataset\n",
        "twenty_train1 = fetch_20newsgroups(subset='train', categories=classes, shuffle=True, random_state=42)\n",
        "twenty_test1 = fetch_20newsgroups(subset='test', categories=classes, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "69p8hintSOJ_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Level: lexicon, model: tf-idf\n",
        "text_clf = Pipeline([\n",
        "    # Text processing steps\n",
        "    ('vect', CountVectorizer(analyzer=processText)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "    # Logistic Regression classifier\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ],
      "metadata": {
        "id": "dXcd53qlSQWe"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "text_clf.fit(twenty_train1.data, twenty_train1.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "uGGzvzr5SfHZ",
        "outputId": "3a675ab7-6c17-4ef1-be4c-de3f754d1f84"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect',\n",
              "                 CountVectorizer(analyzer=<function processText at 0x7e1443a3b5b0>)),\n",
              "                ('tfidf', TfidfTransformer()), ('clf', LogisticRegression())])"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                 CountVectorizer(analyzer=&lt;function processText at 0x7e1443a3b5b0&gt;)),\n",
              "                (&#x27;tfidf&#x27;, TfidfTransformer()), (&#x27;clf&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                 CountVectorizer(analyzer=&lt;function processText at 0x7e1443a3b5b0&gt;)),\n",
              "                (&#x27;tfidf&#x27;, TfidfTransformer()), (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function processText at 0x7e1443a3b5b0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQ8DmPNRUuJ"
      },
      "source": [
        "# Step 4: Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "predicted = text_clf.predict(twenty_test1.data)\n"
      ],
      "metadata": {
        "id": "mBSMZA47TKFz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXHJHqoBmyJ"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier for each class individually\n",
        "precision_alt_atheism = precision_score(twenty_test1.target, predicted, pos_label=0)\n",
        "recall_alt_atheism = recall_score(twenty_test1.target, predicted, pos_label=0)\n",
        "f1_alt_atheism = f1_score(twenty_test1.target, predicted, pos_label=0)\n",
        "\n",
        "precision_sci_med = precision_score(twenty_test1.target, predicted, pos_label=1)\n",
        "recall_sci_med = recall_score(twenty_test1.target, predicted, pos_label=1)\n",
        "f1_sci_med = f1_score(twenty_test1.target, predicted, pos_label=1)"
      ],
      "metadata": {
        "id": "czSz7XqiTRYy"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print evaluation metrics for each class\n",
        "print(\"Class 'alt.atheism':\")\n",
        "print(\"Precision:\", precision_alt_atheism)\n",
        "print(\"Recall:\", recall_alt_atheism)\n",
        "print(\"F1 Score:\", f1_alt_atheism)\n",
        "print(\"\\nClass 'sci.med':\")\n",
        "print(\"Precision:\", precision_sci_med)\n",
        "print(\"Recall:\", recall_sci_med)\n",
        "print(\"F1 Score:\", f1_sci_med)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOFAD3SiTmoR",
        "outputId": "4e7c88c6-8361-4c94-8a2f-8c1de0b6434e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 'alt.atheism':\n",
            "Precision: 0.9787234042553191\n",
            "Recall: 0.8652037617554859\n",
            "F1 Score: 0.9184692179700499\n",
            "\n",
            "Class 'sci.med':\n",
            "Precision: 0.9006928406466512\n",
            "Recall: 0.9848484848484849\n",
            "F1 Score: 0.9408926417370326\n"
          ]
        }
      ]
    }
  ]
}